{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "## initialize openai client\n",
    "client = OpenAI(api_key = os.getenv('OPEN_AI_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def let_gpt_judge_name(name, sql, model):\n",
    "\n",
    "    cte_eval_prompt = f\"\"\"\n",
    "        You will evaluate the comprehensiveness of the name assigned to this common table expression (CTE) from 1 to 10, where:\n",
    "        - 1 indicates a name that is vague or unrelated, and\n",
    "        - 10 indicates a highly descriptive, accurate name that clearly reflects the CTE's purpose.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        1. SQL: SELECT customer_id, SUM(order_total) AS total_spent FROM orders GROUP BY customer_id\n",
    "            - Name: total_spent_per_customer – Expected score: 9\n",
    "            - Name: customer_data – Expected score: 3\n",
    "        2. SQL: SELECT product_id, COUNT(*) AS order_count FROM order_details GROUP BY product_id\n",
    "            - Name: ProductOrderCounts – Expected score: 10\n",
    "            - Name: details – Expected score: 2\n",
    "        3. SQL: SELECT order_id, MAX(order_date) FROM orders GROUP BY order_id\n",
    "            - Name: latest_order_dates – Expected score: 8\n",
    "            - Name: order_info – Expected score: 4\n",
    "\n",
    "        Placeholder names like tmp, aaa, cte1, table1, mycte, etc. are placeholder names that say nothing about the CTE, so they would recieve scores of 1-2.\n",
    "\n",
    "        Assume the following SQL snippet:\n",
    "        ```sql\n",
    "        {sql}\n",
    "        ```\n",
    "        Your task: Evaluate this proposed name for the SQL statement in terms of its comprehensiveness. Provide only a single numeric score from 1 to 10 as output.\n",
    "\n",
    "        Name to evaluate: {name}\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    persona =  \"graduate student in computer science specializing in databases and skilled in SQL\" \n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": f\"You are a {persona}, and you're tasked with evaluating names to common table expressions. The names should reflect what result set the SQL is trying to produce\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": cte_eval_prompt},\n",
    "    ]\n",
    "\n",
    "    model_response = client.chat.completions.create(model=model, messages=messages, max_tokens=50, seed=42, temperature=0) # \"gpt-4o\" or \"gpt-4-turbo\"\n",
    "    \n",
    "    return model_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_gpt_evaluations(json_cte_data_file):\n",
    "    with open(json_cte_data_file, 'r+') as file:\n",
    "        cte_data = json.load(file)\n",
    "\n",
    "        for i, obj in enumerate(cte_data):\n",
    "            if \"gpt-4o-name-evaluation\" not in obj or not obj[\"gpt-4o-name-evaluation\" ]:\n",
    "                try:\n",
    "                    obj[\"gpt-4o-name-evaluation\" ] = let_gpt_judge_name(obj[\"cte_name\"], obj['SQL'], \"gpt-4o\")\n",
    "                    file.seek(0)\n",
    "                    json.dump(cte_data, file, indent=2)\n",
    "                    file.truncate()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating 4o summary for {json_cte_data_file} at index {i}: {e}\")\n",
    "                    # Pause and return to allow rerun later\n",
    "                    return -1\n",
    "            if \"gpt-4-turbo-name-evaluation\" not in obj or not obj[\"gpt-4-turbo-name-evaluation\" ]:\n",
    "                try:\n",
    "                    obj[\"gpt-4-turbo-name-evaluation\" ] = let_gpt_judge_name(obj[\"cte_name\"], obj['SQL'], \"gpt-4-turbo\")\n",
    "                    file.seek(0)\n",
    "                    json.dump(cte_data, file, indent=2)\n",
    "                    file.truncate()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating 4-turbo summary for {json_cte_data_file} at index {i}: {e}\")\n",
    "                    # Pause and return to allow rerun later\n",
    "                    return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data_files/curated_ctesjson'\n",
    "\n",
    "write_in_gpt_evaluations(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
