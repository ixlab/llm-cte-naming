{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient, login\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "login(os.getenv('HF_TOKEN')) # need huggingface token generated from site for account to use certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    # \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    # 'google/gemma-2-9b-it' # Doesn't work with hf_hub==0.24.5 (don't want to remove system message functionality)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def let_llm_judge_name(name, sql, model):\n",
    "\n",
    "    client = InferenceClient(model)\n",
    "    messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"You are a helpful SQL assistant evaluating names to common table expressions. The names should reflect what result set the SQL is trying to produce\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": f\"Consider the following SQL snippet\\n```sql\\n{sql}```\\n\\nEvaluate this name for the SQL statement as a common table expression from 1 to 10, 1 being the least comprehensive, and 10 being the most, return only the nummeric value from 1 to 10 and nothing else:\\n{name}\"},\n",
    "    ]\n",
    "    model_response = client.chat.completions.create(messages=messages, max_tokens=2000, seed=42, temperature=0)\n",
    "    return model_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def write_in_llm_evaluations(json_cte_data_file):\n",
    "    with open(json_cte_data_file, 'r+') as file:\n",
    "        cte_data = json.load(file)\n",
    "\n",
    "        for i, obj in enumerate(cte_data):\n",
    "            for model in models:\n",
    "                if f\"{model.split('/')[1]}-name-evaluation\" not in obj or not obj[f\"{model.split('/')[1]}-name-evaluation\"]:\n",
    "                    try:\n",
    "                        obj[f\"{model.split('/')[1]}-name-evaluation\" ] = let_llm_judge_name(obj[\"cte-name\"], obj['SQL'], model)\n",
    "                        file.seek(0)\n",
    "                        json.dump(cte_data, file, indent=2)\n",
    "                        file.truncate()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating summary for {json_cte_data_file}; model {model} at index {i}: {e}\")\n",
    "                        # Pause and return to allow rerun later\n",
    "                        return -1\n",
    "    return 0\n",
    "\n",
    "def sleep_until_next_hour():\n",
    "    now = datetime.now()\n",
    "    # Calculate the time until the next hour\n",
    "    next_hour = (now + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)\n",
    "    sleep_time = (next_hour - now).total_seconds()\n",
    "    print(f\"Sleeping for {int(sleep_time // 60)} minutes until {next_hour}.\")\n",
    "    time.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = 'data_files/spider2-lite-ctes.json'\n",
    "\n",
    "result = -1\n",
    "while result != 0:\n",
    "    result = write_in_llm_evaluations(json_data)\n",
    "    if result == -1:\n",
    "        sleep_until_next_hour()\n",
    "    elif result == 0:\n",
    "        print(\"Finished successfully.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use informed prompts and personas to judge things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_first_numeric_value(text):\n",
    "    for token in re.split(r'\\s+', text):\n",
    "        if token.isnumeric():\n",
    "            if int(token) in range(0,11):\n",
    "                return int(token)\n",
    "    return None\n",
    "\n",
    "\n",
    "def let_llm_judge_name_as_persona(name, sql, model):\n",
    "\n",
    "    cte_eval_prompt = f\"\"\"\n",
    "        You will evaluate the comprehensiveness of the name assigned to this common table expression (CTE) from 1 to 10, where:\n",
    "        - 1 indicates a name that is vague or unrelated, and\n",
    "        - 10 indicates a highly descriptive, accurate name that clearly reflects the CTE's purpose.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        1. SQL: SELECT customer_id, SUM(order_total) AS total_spent FROM orders GROUP BY customer_id\n",
    "            - Name: total_spent_per_customer – Expected score: 9\n",
    "            - Name: customer_data – Expected score: 3\n",
    "        2. SQL: SELECT product_id, COUNT(*) AS order_count FROM order_details GROUP BY product_id\n",
    "            - Name: ProductOrderCounts – Expected score: 10\n",
    "            - Name: details – Expected score: 2\n",
    "        3. SQL: SELECT order_id, MAX(order_date) FROM orders GROUP BY order_id\n",
    "            - Name: latest_order_dates – Expected score: 8\n",
    "            - Name: order_info – Expected score: 4\n",
    "\n",
    "        Placeholder names like tmp, aaa, cte1, table1, mycte, etc. are placeholder names that say nothing about the CTE, so they would recieve scores of 1-2.\n",
    "\n",
    "        Assume the following SQL snippet:\n",
    "        ```sql\n",
    "        {sql}\n",
    "        ```\n",
    "        Your task: Evaluate this proposed name for the SQL statement in terms of its comprehensiveness. Provide only a single numeric score from 1 to 10 as output.\n",
    "\n",
    "        Name to evaluate: {name}\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "    client = InferenceClient(model)\n",
    "    messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": f\"You are a graduate student in computer science specializing in databases and skilled in SQ, and you're tasked with evaluating names to common table expressions. The names should reflect what result set the SQL is trying to produce\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": cte_eval_prompt},\n",
    "    ]\n",
    "    model_response = client.chat.completions.create(messages=messages, max_tokens=2000, seed=42, temperature=0)\n",
    "    return model_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def write_in_llm_evaluations_both_files(json_cte_data_file):\n",
    "    with open(json_cte_data_file, 'r+') as file:\n",
    "        cte_data = json.load(file)\n",
    "\n",
    "        for i, obj in cte_data:\n",
    "            for model in models:\n",
    "                if f\"{model.split('/')[1]}-name-evaluation\" not in obj or not obj[f\"{model.split('/')[1]}-name-evaluation\"] or extract_first_numeric_value(obj[f\"{model.split('/')[1]}-name-evaluation\"]) is None:\n",
    "                    try:\n",
    "                        obj[f\"{model.split('/')[1]}-name-evaluation\" ] = let_llm_judge_name_as_persona(obj[\"cte-name\"], obj['SQL'], model)\n",
    "                        file.seek(0)\n",
    "                        json.dump(cte_data, file, indent=2)\n",
    "                        file.truncate()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating summary for {json_cte_data_file}; model {model} at index {i}: {e}\")\n",
    "                        # Pause and return to allow rerun later\n",
    "                        return -1\n",
    "    return 0\n",
    "\n",
    "def sleep_until_next_hour():\n",
    "    now = datetime.now()\n",
    "    # Calculate the time until the next hour\n",
    "    next_hour = (now + timedelta(hours=1)).replace(minute=0, second=0, microsecond=0)\n",
    "    sleep_time = (next_hour - now).total_seconds()\n",
    "    print(f\"Sleeping for {int(sleep_time // 60)} minutes until {next_hour}.\")\n",
    "    time.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data = 'data_files/spider2-lite-ctes.json'\n",
    "\n",
    "\n",
    "result = -1\n",
    "while result != 0:\n",
    "    result = write_in_llm_evaluations_both_files(student_data, ds_data)\n",
    "    if result == -1:\n",
    "        sleep_until_next_hour()\n",
    "    elif result == 0:\n",
    "        print(\"Finished successfully.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work on Curated Dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_llm_evaluations_both_files2(json_cte_data_file, json_cte_data_file_2):\n",
    "    with open(json_cte_data_file, 'r+') as file, open(json_cte_data_file_2, 'r+') as file2:\n",
    "        cte_data = json.load(file)\n",
    "        cte_data_2 = json.load(file2)\n",
    "\n",
    "        persona1 = \"graduate student in computer science specializing in databases and skilled in SQL\"\n",
    "        persona2 = \"data scientist who does a lot of data wrangling in SQL and uses temp tables and CTES\"\n",
    "\n",
    "        for i, (obj, obj2) in enumerate(zip(cte_data, cte_data_2)):\n",
    "            for model in models:\n",
    "                if f\"{model.split('/')[1]}-name-evaluation\" not in obj or not obj[f\"{model.split('/')[1]}-name-evaluation\"] or extract_first_numeric_value(obj[f\"{model.split('/')[1]}-name-evaluation\"]) is None:\n",
    "                    try:\n",
    "                        obj[f\"{model.split('/')[1]}-name-evaluation\" ] = let_llm_judge_name_as_persona(obj[\"cte_name\"], obj['SQL'], model, persona1)\n",
    "                        file.seek(0)\n",
    "                        json.dump(cte_data, file, indent=2)\n",
    "                        file.truncate()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating summary for {json_cte_data_file}; model {model} at index {i}: {e}\")\n",
    "                        # Pause and return to allow rerun later\n",
    "                        return -1\n",
    "                if f\"{model.split('/')[1]}-name-evaluation\" not in obj2 or not obj2[f\"{model.split('/')[1]}-name-evaluation\"] or extract_first_numeric_value(obj2[f\"{model.split('/')[1]}-name-evaluation\"]) is None:\n",
    "                    try:\n",
    "                        obj2[f\"{model.split('/')[1]}-name-evaluation\" ] = let_llm_judge_name_as_persona(obj2[\"cte_name\"], obj2['SQL'], model, persona2)\n",
    "                        file2.seek(0)\n",
    "                        json.dump(cte_data_2, file2, indent=2)\n",
    "                        file2.truncate()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating summary for {json_cte_data_file_2}; model {model} at index {i}: {e}\")\n",
    "                        # Pause and return to allow rerun later\n",
    "                        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished successfully.\n"
     ]
    }
   ],
   "source": [
    "student_data = 'data_files/curated/curated_ctes_student_llm_judged.json'\n",
    "ds_data = 'data_files/curated/curated_ctes_ds_llm_judged.json'\n",
    "\n",
    "\n",
    "result = -1\n",
    "while result != 0:\n",
    "    result = write_in_llm_evaluations_both_files2(student_data, ds_data)\n",
    "    if result == -1:\n",
    "        sleep_until_next_hour()\n",
    "    elif result == 0:\n",
    "        print(\"Finished successfully.\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
